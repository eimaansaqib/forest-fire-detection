\section{Method}

In this section, we review how different models have been applied to extract dense features for semantic segmentation on the forest fire images data set.

\subsection{Preprocessing}

To prepare the images and masks for the segmentation task, we resized them to a resolution of 520x520, further using the antialias option on the images to avoid aliasing artifacts. The RGB color channels of the images were normalized using the mean and standard deviation values of [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225], respectively.

\subsection{DeepLab-ResNet}

The first model we used for the segmentation task was a pre-trained DeepLab-ResNet50 \cite{chen_rethinking_2017}. The model was initialized with weights from the COCO data set with VOC labels. We froze the backbone of this pre-trained model, while the classifier head was retrained in order to fine tune the model to our data set. The classifier head was also modified to output a single channel segmentation mask for the input image. The model was retained as includes skip connections, which allow information to bypass certain layers of the network. This can help alleviate the problem of vanishing gradients that can occur in very deep networks such as this one, and can also help preserve spatial information during the segmentation process, leading to higher accuracy on our data set.

Moreover, the DeepLab \cite{chen_rethinking_2017} model head has been optimized for efficient computation. This is achieved by using a parallelized implementation of atrous convolution and a multi-scale feature aggregation technique. As a result, the model can handle large images and produce accurate segmentations in a reasonable amount of time.

We also used the ResNet101 backbone in the same way, in order to achieve better performance with less parameters due to the network being deeper.

\subsection{LRASPP-MobileNet}
The lraspp\_mobilenet\_v3\_large\_FLAME model \cite{howard2019searching} used in this study is a convolution neural network (CNN) based on the MobileNet architecture. This model was also pre-trained on the same COCO data set with VOC labels. The pre-trained backbone of the model was retained. The classifier head was modified with two additional convolution layers to produce fire and not-fire predictions. This is followed by a sigmoid activation to output the final segmentation mask. The classifier head was retrained to fit to our data set. The final architecture of this model enhances the feature representation by incorporating low-level feature maps in a spatial pyramid pooling module with a lightweight attention mechanism, which helps to capture both local and global context. This improves the accuracy of the model in detecting object boundaries and fine-grained details. This also helps strike a fairly decent balance between speed and accuracy, where this model is able to achieve state-of-the-art results while maintaining fast inference times.

The LRASPP \cite{howard2019searching} head we used is a modification of the Spatial Pyramid Pooling (SPP) module that introduces an attention mechanism to weight the importance of different spatial regions. This allows the model to capture more relevant features for the segmentation task and improves the accuracy of the results. By using the LRASPP head, the model can capture both low-level and high-level features efficiently, which leads to accurate segmentations while using fewer computational resources.

\subsection{DeepLab-MobileNet}

We similarly used the DeepLab-MobileNet \cite{chen_rethinking_2017} model for our segmentation task, with modifications to the classifier head. The idea is to harness the ability of DeepLab \cite{chen_rethinking_2017} to handle images with multiple scales. The model head uses atrous (dilated) convolutions to effectively capture multi-scale contextual information, allowing it to produce accurate segmentations at different scales in the image.

\subsection{Segformer}

We fitted a SegFormer model \cite{xie_segformer_2021} on our dataset for the segmentation of forest fire images. The SegFormer is an deep learning model that leverages vision transformers, offering a powerful and efficient approach to semantic segmentaion. The structure of the SegFormer comprises two main parts: a hierarchical Transformer encoder, consisting of a series of Mix Transformer (MiT) encoders \cite{xie_segformer_2021}, and a streamlined All-MLP decoder.

The hierarchical Transformer encoder is designed to create hierarchical features from the input image, resembling the features generated by CNNs. These features capture both fine-grained details at a high-resolution and coarse information at a low-resolution, which are critical for accurate forest fire detection across different scales and situations. The encoder utilizes a technique known as Overlapped Patch Merging to generate hierarchical feature maps while maintaining the local continuity of the image. This approach involves breaking the input image into smaller segments, extracting features from each segment, and merging neighboring segments with some degree of overlap to preserve the local structure.

The All-MLP decoder functions as the second component of the SegFormer architecture. This streamlined decoder is tasked with merging the multi-level features produced by the hierarchical Transformer encoder to create the final semantic segmentation mask. The decoder incorporates multiple MLP layers that efficiently combine the features, resulting in a detailed and precise segmentation representation.

\subsection{Ensemble}

The ensemble model used in this research is a combination of DeepLab-ResNet50 \cite{chen_rethinking_2017} and LRASPP-MobileNet \cite{howard2019searching} models. Both models are pre-trained on the COCO dataset with VOC labels. The DeepLab-ResNet50 model is fine-tuned by modifying its classifier to output a single channel mask. The LRASPP-MobileNet model is also fine-tuned by modifying its low-classifier and high-classifier to output a single channel mask. The modified DeepLab-ResNet50 and LRASPP-MobileNet models are then integrated into an ensemble model. After pretraining the two models, their weights are frozen before training the ensemble model. The ensemble model is more robust to noise and outliers in the input data as it can detect patterns and features that may be missed by the two individual models.

The ensemble model consists of the backbone of both DeepLab-ResNet50 \cite{chen_rethinking_2017} and LRASPP-MobileNet \cite{howard2019searching} models. The output from DeepLab-ResNet50's classifier and LRASPP-MobileNet's classifier are concatenated together to make a 2 channel image where each channel represent the output from one of the model. The concatenated 2 channel image is passed through a 1x1 convolution which merges the output from both the models into one segmentation mask.

The ensemble model is trained end-to-end using the binary cross-entropy loss function. Combining the two models helps to generalize the segmentation results across different types of forest fire images, while speeding up the convergence of the training process.

\subsection{Training}

The models are trained using an Adam optimizer with a batch size of 8 and 3-5 epochs depending on the size of the models. Binary Cross Entropy with Logits loss has been used to train and fine tune the models to the forest fire data set

\subsection{Evaluation}

The performance of the model is evaluated using metrics such as F1-score, and intersection over union (IoU). The test set consists of 400 images, which are not included in the training or validation set. The predictions are compared against the ground truth masks using the aforementioned metrics. The generated masks are also printed out and compared against ground truth masks for visual qualitative evaluation.